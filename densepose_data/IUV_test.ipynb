{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, ClassVar, Dict, List\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from detectron2.config import CfgNode, get_cfg\n",
    "from detectron2.data.detection_utils import read_image\n",
    "from detectron2.engine.defaults import DefaultPredictor\n",
    "from detectron2.structures.instances import Instances\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "from densepose import add_densepose_config\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from densepose.structures import DensePoseChartPredictorOutput, DensePoseEmbeddingPredictorOutput\n",
    "from densepose.utils.logger import verbosity_to_level\n",
    "from densepose.vis.base import CompoundVisualizer\n",
    "from densepose.vis.bounding_box import ScoredBoundingBoxVisualizer\n",
    "from densepose.vis.densepose_outputs_vertex import (\n",
    "    DensePoseOutputsTextureVisualizer,\n",
    "    DensePoseOutputsVertexVisualizer,\n",
    "    get_texture_atlases,\n",
    ")\n",
    "from densepose.vis.densepose_results import (\n",
    "    DensePoseResultsContourVisualizer,\n",
    "    DensePoseResultsFineSegmentationVisualizer,\n",
    "    DensePoseResultsUVisualizer,\n",
    "    DensePoseResultsVVisualizer,\n",
    ")\n",
    "from densepose.vis.densepose_results_textures import (\n",
    "    DensePoseResultsVisualizerWithTexture,\n",
    "    get_texture_atlas,\n",
    ")\n",
    "from densepose.vis.extractor import (\n",
    "    CompoundExtractor,\n",
    "    DensePoseOutputsExtractor,\n",
    "    DensePoseResultExtractor,\n",
    "    create_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from densepose import add_densepose_config\n",
    "from densepose.vis.densepose_results import (\n",
    "    DensePoseResultsFineSegmentationVisualizer as Visualizer,\n",
    ")\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "def setup_config(\n",
    "        config_fpath: str, model_fpath: str, args, opts: List[str]\n",
    "    ):\n",
    "    cfg = get_cfg()\n",
    "    add_densepose_config(cfg)\n",
    "    cfg.merge_from_file(config_fpath)\n",
    "    cfg.merge_from_list(args.opts)\n",
    "    if opts:\n",
    "        cfg.merge_from_list(opts)\n",
    "    cfg.MODEL.WEIGHTS = model_fpath\n",
    "    cfg.freeze()\n",
    "    return cfg\n",
    "\n",
    "def _get_input_file_list(input_spec: str):\n",
    "        if os.path.isdir(input_spec):\n",
    "            file_list = [\n",
    "                os.path.join(input_spec, fname)\n",
    "                for fname in os.listdir(input_spec)\n",
    "                if os.path.isfile(os.path.join(input_spec, fname))\n",
    "            ]\n",
    "        elif os.path.isfile(input_spec):\n",
    "            file_list = [input_spec]\n",
    "        else:\n",
    "            file_list = glob.glob(input_spec)\n",
    "        return file_list\n",
    "\n",
    "def _get_out_fname(entry_idx: int, fname_base: str):\n",
    "    base, ext = os.path.splitext(fname_base)\n",
    "    return base + \".{0:04d}\".format(entry_idx) + ext\n",
    "\n",
    "class ARGS(object):\n",
    "    def __init__(self) -> None:\n",
    "        self.model = 'densepose_rcnn_R_101_FPN_DL_WC1M_s1x.pkl'\n",
    "        self.cfg = 'densepose_rcnn_R_101_FPN_DL_WC1M_s1x.yaml'\n",
    "        self.input = 'ref.png'\n",
    "        self.opts = []\n",
    "    \n",
    "args = ARGS()\n",
    "# opts = []\n",
    "# cfg = setup_config(args.cfg, args.model, args, opts)\n",
    "cfg = get_cfg()\n",
    "add_densepose_config(cfg)\n",
    "cfg.merge_from_file(\"densepose_rcnn_R_101_FPN_DL_WC1M_s1x.yaml\")\n",
    "cfg.MODEL.WEIGHTS = 'densepose_rcnn_R_101_FPN_DL_WC1M_s1x.pkl'\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "file_list = _get_input_file_list(args.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1333\n"
     ]
    }
   ],
   "source": [
    "print(cfg.INPUT.MIN_SIZE_TEST)\n",
    "print(cfg.INPUT.MAX_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mpixel_mean\u001b[0m\n",
      "  \u001b[35mpixel_std\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import detectron2.data.transforms as T\n",
    "class DefaultPredictor:\n",
    "    \"\"\"\n",
    "    Create a simple end-to-end predictor with the given config that runs on\n",
    "    single device for a single input image.\n",
    "\n",
    "    Compared to using the model directly, this class does the following additions:\n",
    "\n",
    "    1. Load checkpoint from `cfg.MODEL.WEIGHTS`.\n",
    "    2. Always take BGR image as the input and apply conversion defined by `cfg.INPUT.FORMAT`.\n",
    "    3. Apply resizing defined by `cfg.INPUT.{MIN,MAX}_SIZE_TEST`.\n",
    "    4. Take one input image and produce a single output, instead of a batch.\n",
    "\n",
    "    This is meant for simple demo purposes, so it does the above steps automatically.\n",
    "    This is not meant for benchmarks or running complicated inference logic.\n",
    "    If you'd like to do anything more complicated, please refer to its source code as\n",
    "    examples to build and use the model manually.\n",
    "\n",
    "    Attributes:\n",
    "        metadata (Metadata): the metadata of the underlying dataset, obtained from\n",
    "            cfg.DATASETS.TEST.\n",
    "\n",
    "    Examples:\n",
    "    ::\n",
    "        pred = DefaultPredictor(cfg)\n",
    "        inputs = cv2.imread(\"input.jpg\")\n",
    "        outputs = pred(inputs)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg.clone()  # cfg can be modified by model\n",
    "        self.model = build_model(self.cfg)\n",
    "        self.model.eval()\n",
    "\n",
    "        checkpointer = DetectionCheckpointer(self.model)\n",
    "        checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "        self.aug = T.ResizeShortestEdge(\n",
    "            [0, 0], 1333\n",
    "        )\n",
    "\n",
    "        self.input_format = cfg.INPUT.FORMAT\n",
    "        assert self.input_format in [\"RGB\", \"BGR\"], self.input_format\n",
    "\n",
    "    def __call__(self, original_image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            original_image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n",
    "\n",
    "        Returns:\n",
    "            predictions (dict):\n",
    "                the output of the model for one image only.\n",
    "                See :doc:`/tutorials/models` for details about the format.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "            # Apply pre-processing to image.\n",
    "            if self.input_format == \"RGB\":\n",
    "                # whether the model expects BGR inputs or RGB\n",
    "                original_image = original_image[:, :, ::-1]\n",
    "            height, width = original_image.shape[:2]\n",
    "            image = self.aug.get_transform(original_image).apply_image(original_image)\n",
    "            # image = original_image\n",
    "            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "            image.to(self.cfg.MODEL.DEVICE)\n",
    "\n",
    "            # inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "            inputs = {\"image\": image}\n",
    "            \n",
    "            predictions = self.model([inputs])\n",
    "            return predictions\n",
    "\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from densepose.vis.extractor import DensePoseResultExtractor\n",
    "\n",
    "vis_I = DensePoseResultsFineSegmentationVisualizer()\n",
    "# ext_I = create_extractor(vis_I)\n",
    "ext_I = DensePoseResultExtractor()\n",
    "\n",
    "vis_U = DensePoseResultsUVisualizer()\n",
    "ext_U = create_extractor(vis_U)\n",
    "\n",
    "vis_V = DensePoseResultsVVisualizer()\n",
    "ext_V = create_extractor(vis_V)\n",
    "\n",
    "vis = [vis_I, vis_U, vis_V]\n",
    "ext = [ext_I, ext_U, ext_V]\n",
    "\n",
    "dataset_dir = '/data1/lihaochen/TikTok_finetuning/TiktokDance'\n",
    "# split = 'train_images'\n",
    "split = 'new10val_images'\n",
    "\n",
    "def tsv_reader(tsv_file, sep='\\t'):\n",
    "    with open(tsv_file, 'r') as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            yield [x.strip() for x in line.split(sep)]\n",
    "\n",
    "tsv_fname_img = dataset_dir + f'/{split}.tsv'\n",
    "tsv_imgs = tsv_reader(tsv_fname_img)\n",
    "\n",
    "# for file_name in file_list:\n",
    "#     img = read_image(file_name, format=\"BGR\")  # predictor expects BGR image.\n",
    "\n",
    "for i, img_row in enumerate(tsv_imgs):\n",
    "    import base64\n",
    "    image_key = img_row[0]\n",
    "    if image_key != 'TiktokDance_201_005_1x1_00073.jpg':\n",
    "        continue\n",
    "    file_name = image_key\n",
    "    image = cv2.imdecode(np.frombuffer(base64.b64decode(img_row[1]), np.uint8),cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # add_image = Image.open(file_name)\n",
    "    # img_tensor = transforms.ToTensor()\n",
    "    # add_image = img_tensor(add_image)\n",
    "    # add_image = (add_image - 0.5) * 2\n",
    "    # noise = torch.randn([224, 224]).unsqueeze(0).repeat(3, 1, 1).clamp(-1.0, 1.0)\n",
    "    # noise_img = 0.7*add_image + 0.3*noise\n",
    "    # noise_img = (noise_img / 2) - 0.5\n",
    "    # noise_img = noise_img.cpu().permute(1, 2, 0).float().numpy()\n",
    "    # noise_img = (noise_img * 255).round().astype(\"uint8\")\n",
    "    # noise_img = Image.fromarray(noise_img)\n",
    "    # noise_img.save('noisy.png')\n",
    "    with torch.no_grad():\n",
    "        outputs = predictor(img)\n",
    "        for i in range(1):\n",
    "            output = outputs[i][\"instances\"]\n",
    "            datas = []\n",
    "            idx = 0\n",
    "            for e in ext:\n",
    "                datas.append(e(output))\n",
    "\n",
    "            for data, v in zip(datas, vis):\n",
    "                image_vis = v.visualize(np.zeros_like(img), data)\n",
    "                entry_idx_ = idx + 1\n",
    "                out_fname = _get_out_fname(entry_idx_, file_name)\n",
    "                out_dir = os.path.dirname(out_fname)\n",
    "                if len(out_dir) > 0 and not os.path.exists(out_dir):\n",
    "                    os.makedirs(out_dir)\n",
    "                cv2.imwrite(out_fname, image_vis)\n",
    "                idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DisCo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
